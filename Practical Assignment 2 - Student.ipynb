{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NuJ0879WJF0E",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Names: ['namestudent1', 'namestudent2'] \n",
    "Studentnumbers: ['studentnumberstudent1', 'studentnumberstudent2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PHjzAhvgBlo7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1cj-CzkY6QZUe42ky64GI5CSSg7-K40N5\n",
      "To: /Users/daanschoppink/Downloads/loan_default_10K.csv\n",
      "100%|██████████████████████████████████████| 48.3M/48.3M [00:02<00:00, 17.3MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1MbWGXLawE3VTxP1XgNpj8uEo1VHPq12B\n",
      "To: /Users/daanschoppink/Downloads/loan_default_100K.csv\n",
      "100%|████████████████████████████████████████| 484M/484M [00:29<00:00, 16.2MB/s]\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sklearn in /usr/local/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.22.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.8.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.22.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "# Dataset with 10K instances\n",
    "! gdown \"https://drive.google.com/uc?id=1cj-CzkY6QZUe42ky64GI5CSSg7-K40N5\"\n",
    "# Dataset with 100k instances\n",
    "! gdown \"https://drive.google.com/uc?id=1MbWGXLawE3VTxP1XgNpj8uEo1VHPq12B\"\n",
    "! pip install sklearn\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SlHJBPXQJF0K",
    "nbgrader": {
     "checksum": "86f687db575cf409d54ac5e91e6dd861",
     "grade": false,
     "grade_id": "cell-1979a89473cf7ece",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 1: Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wKpbQMIQJF0K",
    "nbgrader": {
     "checksum": "89914f38bf3510543caaef66d0f14169",
     "grade": false,
     "grade_id": "cell-700f4a41782fa412",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Applied Machine learning\n",
    "## Practical Assignment 2\n",
    "\n",
    "### Important Notes:\n",
    "1. Submit through **Canvas** before 11:59pm on Tuesday, May 17, 2022\n",
    "2. No late homework will be accepted\n",
    "3. This is a group-of-two assignment\n",
    "4. The submitted file should be in ipynb format\n",
    "5. The assignment is worth it 10 points\n",
    "6. For questions, please use the discussion part of Canvas (English only!)\n",
    "7. The indication **optional** means that the question is optional; you won't lose any points if you do not do that part of the assignment, nor will you gain if you do it.\n",
    "\n",
    "### Software:\n",
    "We will be using Python programming language throughout this course. Further we will be using:\n",
    "+ IPython Notebooks (as an environment)\n",
    "+ Numpy\n",
    "+ Pandas\n",
    "+ Scikit-learn\n",
    "\n",
    "### Background:\n",
    "\n",
    "This practical assignment will be covering logistic regression, neural networks, support vector machines and evaluation of classifiers. \n",
    "\n",
    "For the assignment, please download a dataset on Load Defaults. You are provided with two datasets:\n",
    "1. [Dataset](https://drive.google.com/open?id=1cj-CzkY6QZUe42ky64GI5CSSg7-K40N5) with 10,000 instances \n",
    "2. [Dataset](https://drive.google.com/open?id=1MbWGXLawE3VTxP1XgNpj8uEo1VHPq12B) with 100,000 instances\n",
    "\n",
    "In principle you should work on the second, larger dataset, but if you face scaling computational issues then better work with the first, smaller dataset.\n",
    "\n",
    "This data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. You are provided with thousands of observations and nearly 800 features. Each observation (instance) is independent from the previous. \n",
    "\n",
    "For each observation, it was recorded whether a default was triggered (i.e. whether an individual could not pay back the loan). In case of a default, the loss for the bank was measured. This quantity lies between 0 and 100. If the loan did not default, the loss for the bank was 0. You are asked to predict whether a loan will default or not for each individual in the test set.\n",
    "\n",
    "Missing feature values have been kept as is, so that the competing teams can really use the maximum data available, implementing a strategy to fill the gaps if desired. Consider all variables continuous, even though some variables may be categorical (e.g. f776 and f777).\n",
    "\n",
    "The goal of the machine learning algorithm will be to predict whether a loan will default, given a set of features. For privacy reasons the feature names are not provided.\n",
    "\n",
    "**Important Note**: This second assignment is not as instructive as the first assignment. The first assignment guided you step-by-step through all the preprocessing, training-validation-testing setup, etc. This assignment does not do so, but it leaves it up to you to decide how to use the data and design your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "k63PQb_KJF0L",
    "nbgrader": {
     "checksum": "cdc9100b2eb32f795318f88531439408",
     "grade": false,
     "grade_id": "cell-3a36fe2e430fa9ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-2fe94ea43ea5>:8: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only\n",
      "  dfn = df.dropna(0, how='any')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('loan_default_10K.csv', sep=\",\", header=0, dtype=np.float64)\n",
    "\n",
    "# Drop the observations that contain missing values\n",
    "dfn = df.dropna(0, how='any')\n",
    "\n",
    "# Consider only a handful of features to start with; you can extend to the full set later on.\n",
    "X = dfn.loc[:,'f400':'f500'].values\n",
    "\n",
    "# Generate the labels; if 'loss' is zero the this indicates the negative class, class 0, i.e. no default;\n",
    "# if 'loss' is possitive this indicates the positive class, class 1, i.e. there is a loan default;\n",
    "y = [ bool(y) for y in dfn.loc[:,'loss'].values ]\n",
    "\n",
    "# Split the data into train, validation, and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "781\n"
     ]
    }
   ],
   "source": [
    "test_pred = [True, True, False, True, False, False, True, True, True, True, True, True]\n",
    "test_true = [True, False, False, True, True, False, False, True, False, False, False, False]\n",
    "\n",
    "print(y_test.count(True))\n",
    "print(y_test.count(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0LKlXxS3JF0L",
    "nbgrader": {
     "checksum": "ff730342a91fefa515c3117b502aa292",
     "grade": false,
     "grade_id": "cell-735be096b0f642ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 2: Evaluation measures (2pts)\n",
    "In what follows you should implement a number of evaluation measures. You need to implement these from scratch, meaning that it is not allowed to call any scikit-learn function, or any other API function that implements the method for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4yvWH4cgJF0L",
    "nbgrader": {
     "checksum": "cdadfd696f9d159f0242cad28b7f41ef",
     "grade": false,
     "grade_id": "cell-a127e8413f617d64",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that produces the contigency matrix, i.e. True Positives, False Positives, True Negatives, False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "id": "294-dY4BJF0M",
    "nbgrader": {
     "checksum": "bd892db60c7761bace1ca06b9361a045",
     "grade": false,
     "grade_id": "cell-aab0f82d6a21bea5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def contigency_matrix(true_y, predicted_y):\n",
    "    # YOUR CODE HERE, Create TP, FP, TN, FN\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    for i, j in zip(true_y, predicted_y):\n",
    "        TP += 1 if (i == True and i == j) else 0\n",
    "        FP += 1 if (i == False and j == True) else 0\n",
    "        TN += 1 if (i == False and i == j) else 0\n",
    "        FN += 1 if (i == True and j == False) else 0\n",
    "    \n",
    "    return np.array(([TP, FP], [TN, FN]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cV3BfX8jJF0M",
    "nbgrader": {
     "checksum": "b2f901219adca9d441082d3fcf702e6b",
     "grade": false,
     "grade_id": "cell-b3c52de1970e361e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes accuracy (without using any built-in accuracy function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "id": "DK2lCwiNJF0M",
    "nbgrader": {
     "checksum": "f6705877fef7cd74d89832d32a8536a0",
     "grade": false,
     "grade_id": "cell-2e0cc734628dd4c2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(true_y, predicted_y):\n",
    "    M = contigency_matrix(true_y, predicted_y)\n",
    "    TP, FP, TN, FN = M[0,0], M[0,1], M[1,0], M[1,1]\n",
    "    \n",
    "    return (TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4LznpIdTJF0M",
    "nbgrader": {
     "checksum": "4b3798ba720235065011afa2736346a7",
     "grade": false,
     "grade_id": "cell-d045e95a552112ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes precision for one class (without using any built-in precision function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "id": "QC8SeWOIJF0M",
    "nbgrader": {
     "checksum": "40f4c7470d6073739cff4934761469c8",
     "grade": false,
     "grade_id": "cell-a403be8ac0ee7af0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def precision(true_y, predicted_y):\n",
    "    M = contigency_matrix(true_y, predicted_y)\n",
    "    TP, FP = M[0,0], M[0,1]\n",
    "    \n",
    "    return TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_D7OYnK4JF0N",
    "nbgrader": {
     "checksum": "6dbe4298af11247615077250b24d3f59",
     "grade": false,
     "grade_id": "cell-4822b32d0cedb0e8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes recall for one class (without using any built-in recall function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "id": "X08ucQRFJF0N",
    "nbgrader": {
     "checksum": "cf41435f0b5003e31ae61340d3188bde",
     "grade": false,
     "grade_id": "cell-075963e37ff41c66",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def recall(true_y, predicted_y):\n",
    "    M = contigency_matrix(true_y, predicted_y)\n",
    "    TP, FN = M[0,0], M[1,1]\n",
    "    \n",
    "    return TP / (TP + FN)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yE78OP9tJF0N",
    "nbgrader": {
     "checksum": "c0fa7903c66407fa515b5c4c5e12b266",
     "grade": false,
     "grade_id": "cell-f0c5ff30db6fc1b0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes f1 for one class(without using any built-in f1 function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "id": "FB-f5OO1JF0N",
    "nbgrader": {
     "checksum": "0b8095ecb0d05692b4a57cc17eff026d",
     "grade": false,
     "grade_id": "cell-bcc41b9d876ee5d4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def f1(true_y, predicted_y):\n",
    "    p = precision(true_y, predicted_y)\n",
    "    r = recall(true_y, predicted_y)\n",
    "    \n",
    "    return 2 * ((p * r) / (p + r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jO5VhQLWJF0N",
    "nbgrader": {
     "checksum": "e0d9785e1e45e2179c318aec035059ad",
     "grade": false,
     "grade_id": "cell-c21fd73cbce64a50",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 3: Algorithms\n",
    "Compare the performance of Logistic Regression, SVMs and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UWXjVAenJF0N",
    "nbgrader": {
     "checksum": "af0943a520201c48990805ce5cb2cb7c",
     "grade": false,
     "grade_id": "cell-7ea6f44ccf633c76",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Logistic Regression (Lecture 3) (2pts)\n",
    "\n",
    "+ Train and test a logistic regression model\n",
    "    + Construct a table with each row being a different value of the regularization parameter and each column the aforementioned measures\n",
    "    + Explain your findings and select the optimal model\n",
    "    + Report the performance of the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "id": "vnnZ6T1MJF0O",
    "nbgrader": {
     "checksum": "bb4b24cb9b3769517aafa02dce6321d4",
     "grade": true,
     "grade_id": "cell-eea85664ef370cd5",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "train_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = train_scaler.transform(X_train)\n",
    "\n",
    "test_scaler = StandardScaler().fit(X_test)\n",
    "X_test_scaled = test_scaler.transform(X_test)\n",
    "\n",
    "# X_train = StandardScaler().fit_transform(X_train)\n",
    "# X_test = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def create_LG(X, y, c: float = 1.0, balance = None):\n",
    "    return LogisticRegression(max_iter = 5000, C = c,  class_weight= balance).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n",
      "<ipython-input-22-4707800dd009>:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 2 * ((p * r) / (p + r))\n"
     ]
    }
   ],
   "source": [
    "c_list = np.arange(0.1, 5.0, 0.1)\n",
    "M = []\n",
    "M_balanced = []\n",
    "for c in c_list:\n",
    "    model = [create_LG(X_train_scaled, y_train, c, None), \n",
    "             create_LG(X_train_scaled, y_train, c, 'balanced')]\n",
    "    \n",
    "    y_pred = [model[0].predict(X_test_scaled),model[1].predict(X_test_scaled)] \n",
    "    \n",
    "    acc = [accuracy(y_test, y_pred[0]), accuracy(y_test, y_pred[1])]\n",
    "    pre = [precision(y_test, y_pred[0]), precision(y_test, y_pred[1])]\n",
    "    rec = [recall(y_test, y_pred[0]), recall(y_test, y_pred[1])]\n",
    "    f1s = [f1(y_test, y_pred[0]), f1(y_test, y_pred[1])]\n",
    "    \n",
    "    M.append([c, acc[0], pre[0], rec[0] ,f1s[0]])\n",
    "    M_balanced.append([c, acc[1], pre[1], rec[1] ,f1s[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalanced data: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reg param</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.7</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.8</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.9</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.1</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.2</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.3</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.4</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.6</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.7</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.8</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.9</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.1</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.2</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.3</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.4</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.6</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3.7</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.8</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3.9</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.1</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.2</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.3</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.6</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.7</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.8</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reg param  Accuracy  Precision  Recall  f1\n",
       "0         0.1  0.893593        NaN     0.0 NaN\n",
       "1         0.2  0.893593        NaN     0.0 NaN\n",
       "2         0.3  0.893593        NaN     0.0 NaN\n",
       "3         0.4  0.893593        NaN     0.0 NaN\n",
       "4         0.5  0.893593        NaN     0.0 NaN\n",
       "5         0.6  0.893593        NaN     0.0 NaN\n",
       "6         0.7  0.893593        NaN     0.0 NaN\n",
       "7         0.8  0.893593        NaN     0.0 NaN\n",
       "8         0.9  0.893593        NaN     0.0 NaN\n",
       "9         1.0  0.893593        NaN     0.0 NaN\n",
       "10        1.1  0.893593        NaN     0.0 NaN\n",
       "11        1.2  0.893593        NaN     0.0 NaN\n",
       "12        1.3  0.893593        NaN     0.0 NaN\n",
       "13        1.4  0.893593        NaN     0.0 NaN\n",
       "14        1.5  0.893593        NaN     0.0 NaN\n",
       "15        1.6  0.893593        NaN     0.0 NaN\n",
       "16        1.7  0.893593        NaN     0.0 NaN\n",
       "17        1.8  0.893593        NaN     0.0 NaN\n",
       "18        1.9  0.893593        NaN     0.0 NaN\n",
       "19        2.0  0.893593        NaN     0.0 NaN\n",
       "20        2.1  0.893593        NaN     0.0 NaN\n",
       "21        2.2  0.893593        NaN     0.0 NaN\n",
       "22        2.3  0.893593        NaN     0.0 NaN\n",
       "23        2.4  0.893593        NaN     0.0 NaN\n",
       "24        2.5  0.893593        NaN     0.0 NaN\n",
       "25        2.6  0.893593        NaN     0.0 NaN\n",
       "26        2.7  0.892449        0.0     0.0 NaN\n",
       "27        2.8  0.892449        0.0     0.0 NaN\n",
       "28        2.9  0.892449        0.0     0.0 NaN\n",
       "29        3.0  0.892449        0.0     0.0 NaN\n",
       "30        3.1  0.892449        0.0     0.0 NaN\n",
       "31        3.2  0.892449        0.0     0.0 NaN\n",
       "32        3.3  0.892449        0.0     0.0 NaN\n",
       "33        3.4  0.892449        0.0     0.0 NaN\n",
       "34        3.5  0.892449        0.0     0.0 NaN\n",
       "35        3.6  0.892449        0.0     0.0 NaN\n",
       "36        3.7  0.892449        0.0     0.0 NaN\n",
       "37        3.8  0.892449        0.0     0.0 NaN\n",
       "38        3.9  0.892449        0.0     0.0 NaN\n",
       "39        4.0  0.892449        0.0     0.0 NaN\n",
       "40        4.1  0.892449        0.0     0.0 NaN\n",
       "41        4.2  0.892449        0.0     0.0 NaN\n",
       "42        4.3  0.892449        0.0     0.0 NaN\n",
       "43        4.4  0.892449        0.0     0.0 NaN\n",
       "44        4.5  0.892449        0.0     0.0 NaN\n",
       "45        4.6  0.892449        0.0     0.0 NaN\n",
       "46        4.7  0.892449        0.0     0.0 NaN\n",
       "47        4.8  0.892449        0.0     0.0 NaN\n",
       "48        4.9  0.892449        0.0     0.0 NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Balanced data: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reg param</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.582380</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.234801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.581236</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.234310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.576659</td>\n",
       "      <td>0.143959</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.232365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.572082</td>\n",
       "      <td>0.142494</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.230453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.567506</td>\n",
       "      <td>0.141058</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.228571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.561785</td>\n",
       "      <td>0.141089</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.229376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.557208</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.224449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.551487</td>\n",
       "      <td>0.136253</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.546911</td>\n",
       "      <td>0.131387</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.544622</td>\n",
       "      <td>0.130751</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.213439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.545767</td>\n",
       "      <td>0.131068</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.213861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.544622</td>\n",
       "      <td>0.130751</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.213439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.544622</td>\n",
       "      <td>0.130751</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.213439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.544622</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.216535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.132212</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.216110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.545767</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.220039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.7</td>\n",
       "      <td>0.542334</td>\n",
       "      <td>0.133652</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.8</td>\n",
       "      <td>0.541190</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.218324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.9</td>\n",
       "      <td>0.540046</td>\n",
       "      <td>0.133017</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.541190</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.218324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.1</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.2</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.3</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.4</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.540046</td>\n",
       "      <td>0.133017</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.6</td>\n",
       "      <td>0.540046</td>\n",
       "      <td>0.133017</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.7</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.8</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.9</td>\n",
       "      <td>0.540046</td>\n",
       "      <td>0.133017</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.1</td>\n",
       "      <td>0.537757</td>\n",
       "      <td>0.132388</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.2</td>\n",
       "      <td>0.536613</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.3</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.4</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.6</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3.7</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.215385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.8</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3.9</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.212355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.1</td>\n",
       "      <td>0.530892</td>\n",
       "      <td>0.128806</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.2</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.3</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.6</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.212355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.7</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.8</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.215385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.215385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reg param  Accuracy  Precision    Recall        f1\n",
       "0         0.1  0.582380   0.145833  0.602151  0.234801\n",
       "1         0.2  0.581236   0.145455  0.602151  0.234310\n",
       "2         0.3  0.576659   0.143959  0.602151  0.232365\n",
       "3         0.4  0.572082   0.142494  0.602151  0.230453\n",
       "4         0.5  0.567506   0.141058  0.602151  0.228571\n",
       "5         0.6  0.561785   0.141089  0.612903  0.229376\n",
       "6         0.7  0.557208   0.137931  0.602151  0.224449\n",
       "7         0.8  0.551487   0.136253  0.602151  0.222222\n",
       "8         0.9  0.546911   0.131387  0.580645  0.214286\n",
       "9         1.0  0.544622   0.130751  0.580645  0.213439\n",
       "10        1.1  0.545767   0.131068  0.580645  0.213861\n",
       "11        1.2  0.544622   0.130751  0.580645  0.213439\n",
       "12        1.3  0.544622   0.130751  0.580645  0.213439\n",
       "13        1.4  0.544622   0.132530  0.591398  0.216535\n",
       "14        1.5  0.543478   0.132212  0.591398  0.216110\n",
       "15        1.6  0.545767   0.134615  0.602151  0.220039\n",
       "16        1.7  0.542334   0.133652  0.602151  0.218750\n",
       "17        1.8  0.541190   0.133333  0.602151  0.218324\n",
       "18        1.9  0.540046   0.133017  0.602151  0.217899\n",
       "19        2.0  0.541190   0.133333  0.602151  0.218324\n",
       "20        2.1  0.538902   0.132701  0.602151  0.217476\n",
       "21        2.2  0.538902   0.132701  0.602151  0.217476\n",
       "22        2.3  0.538902   0.132701  0.602151  0.217476\n",
       "23        2.4  0.538902   0.132701  0.602151  0.217476\n",
       "24        2.5  0.540046   0.133017  0.602151  0.217899\n",
       "25        2.6  0.540046   0.133017  0.602151  0.217899\n",
       "26        2.7  0.538902   0.132701  0.602151  0.217476\n",
       "27        2.8  0.538902   0.132701  0.602151  0.217476\n",
       "28        2.9  0.540046   0.133017  0.602151  0.217899\n",
       "29        3.0  0.538902   0.132701  0.602151  0.217476\n",
       "30        3.1  0.537757   0.132388  0.602151  0.217054\n",
       "31        3.2  0.536613   0.132075  0.602151  0.216634\n",
       "32        3.3  0.535469   0.131765  0.602151  0.216216\n",
       "33        3.4  0.535469   0.131765  0.602151  0.216216\n",
       "34        3.5  0.535469   0.131765  0.602151  0.216216\n",
       "35        3.6  0.535469   0.131765  0.602151  0.216216\n",
       "36        3.7  0.533181   0.131148  0.602151  0.215385\n",
       "37        3.8  0.535469   0.131765  0.602151  0.216216\n",
       "38        3.9  0.535469   0.131765  0.602151  0.216216\n",
       "39        4.0  0.533181   0.129412  0.591398  0.212355\n",
       "40        4.1  0.530892   0.128806  0.591398  0.211538\n",
       "41        4.2  0.532037   0.129108  0.591398  0.211946\n",
       "42        4.3  0.532037   0.129108  0.591398  0.211946\n",
       "43        4.4  0.532037   0.129108  0.591398  0.211946\n",
       "44        4.5  0.532037   0.129108  0.591398  0.211946\n",
       "45        4.6  0.533181   0.129412  0.591398  0.212355\n",
       "46        4.7  0.532037   0.129108  0.591398  0.211946\n",
       "47        4.8  0.533181   0.131148  0.602151  0.215385\n",
       "48        4.9  0.533181   0.131148  0.602151  0.215385"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "LG_table = pd.DataFrame(M, columns = ['Reg param','Accuracy','Precision', 'Recall', 'f1'])\n",
    "LG_table_balanced = pd.DataFrame(M_balanced, columns = ['Reg param','Accuracy','Precision', 'Recall', 'f1'])\n",
    "\n",
    "print(\"Imbalanced data: \")\n",
    "display(LG_table)\n",
    "print(\"\\n\")\n",
    "print(\"Balanced data: \")\n",
    "display(LG_table_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "acc_col = LG_table_balanced[\"Accuracy\"]\n",
    "pre_col = LG_table_balanced[\"Precision\"]\n",
    "re_col = LG_table_balanced[\"Recall\"]\n",
    "f1_col = LG_table_balanced['f1']\n",
    "print(acc_col.idxmax())\n",
    "print(pre_col.idxmax())\n",
    "print(re_col.idxmax())\n",
    "print(f1_col.idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KNigDLF2JF0O",
    "nbgrader": {
     "checksum": "0d74b83486a9bcdcbdf539376bd56e52",
     "grade": false,
     "grade_id": "cell-f57d340c8ac3f3b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Explain what you observe regarding the positive class; i.e. the performance of the algorithm in predicting defaults. Explain why is this happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "NjRu6n9kJF0O",
    "nbgrader": {
     "checksum": "106a108640959c5f8b9523216dfc1ca2",
     "grade": true,
     "grade_id": "cell-75527f6a11293f5c",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Input is rarely classefied as the positive class, this is due to the data set being imbalanced in favor of the negative class. If no balance method is used, calculating the aforementioned measures result in NaN outputs. LogisticRegression() from Sklearn offers a paramter, class_weight,  to balance the weights, partially migating the imbalance in the data set.\n",
    "\n",
    "Looking at both tables, unbalanced and balanced, the optimal model is a model with a low regularization parameter. Testing with different regularization parameter intervals it also showed that a low regularization parameter resulted in better performace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MZUnFZUxJF0O",
    "nbgrader": {
     "checksum": "847a1e00ad9d4316b9512efe1a66df26",
     "grade": false,
     "grade_id": "cell-f1d84ada7bb6859e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "There are a number of ways to fix the problem you have observed above. Here we will consider two of them: downsampling and upsampling. In an ideal situation you will like your dataset to be balanced, i.e. to have the same number of instances for the positive and the negative class.\n",
    "\n",
    "**Downsampling**: Let's assume that the positive class has *n1* instances, while the negative class *n2* instances, where *n2* is much bigger than *n1*. One solution is to create a new training set for which from the *n2* instances of the negative class you sample *n1* of them only to include in your training set; hence now you have *n1* + *n1* training instances.\n",
    "\n",
    "**Upsampling**: Let's assume that the positive class has *n1* instances, while the negative class *n2* instances, where *n2* is much bigger than *n1*. Another solution is to create a new training set for which you create  *n2* instances of the positive class. To do so you sample *n2* instances from the *n1* instance, with replacement. With replacement means that you allow the same instance to be sampled multiple times; hence now you have *n2* + *n2* training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "otj4Qm3MJF0O"
   },
   "source": [
    "#### Downsampling (OPTIONAL – If you wish to skip downsampling continue to Neural Networks further below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_sDkkIcUJF0O",
    "nbgrader": {
     "checksum": "ea6ec4f362d483b9439040f12d09b46a",
     "grade": false,
     "grade_id": "cell-59aa6ae849b90374",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function for downsampling (**optional**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "id": "7L6yUCajJF0P",
    "nbgrader": {
     "checksum": "005c2fee650b5518f0d377e26d46615d",
     "grade": true,
     "grade_id": "cell-f111bc027ec54669",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-15-2ac96c953aaa>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-2ac96c953aaa>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    # your code goes here\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def downsample(y_train):\n",
    "    # y_train is the 1d matrix of the labels in your training data, e.g.\n",
    "    #       0     1     2     3     4   5     6     7     8   ... \n",
    "    # y = [True False False False True True False False False ... False]\n",
    "    #\n",
    "    # the function returns the position of the training data to be considered for the final training set.\n",
    "    # e.g. if you decide from the True instances to select 0, 4 and 5, while from the False instances 1, 3, and 8\n",
    "    # the outcome of the function will be [0, 1, 3, 4, 5, 8] (= sampled_indexes)\n",
    "    \n",
    "    # your code goes here\n",
    "    \n",
    "    return sampled_indexes\n",
    "    \n",
    "def new_training_set(X_train, y_train, sampled_indexes):\n",
    "    # your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "akjEu_5-JF0P",
    "nbgrader": {
     "checksum": "7acfd4fa50b0ce7567f3622b8df52dc9",
     "grade": false,
     "grade_id": "cell-0489481a9fc804d9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Test the performance of logistic regression using the new training set, and report your conclusions (**optional**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "gYWgW2LOJF0P",
    "nbgrader": {
     "checksum": "8ef852228cb6d7212a28012d7e335ad1",
     "grade": true,
     "grade_id": "cell-cd7143b6e088d522",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT1gcqt_JF0P"
   },
   "source": [
    "# The last few questions below are not optional!\n",
    "If you did not finish the optional downsampling, just go through with the data created before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5DL60WojJF0Q",
    "nbgrader": {
     "checksum": "88788580e9ea0cc74560a55c6231466b",
     "grade": false,
     "grade_id": "cell-d7e21719abffa28b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### SVMs (Lecture 4) (2pts)\n",
    "\n",
    "+ Train and test a Support Vector Machine model\n",
    "    + Construct a table with each row being a different configuration of the SVM algorithm (play with the regularization parameter, and the kernel function – use linear, poly, and rbf) and each column the evaluation measures\n",
    "    + Explain your findings and select the optimal model\n",
    "    + Report the performance of the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "id": "Ac5mNYD3JF0P",
    "nbgrader": {
     "checksum": "167eed2eaafde3d4951aef9059e06a58",
     "grade": true,
     "grade_id": "cell-c6f93c6c6f633c47",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def create_SVM(X, y, k, c: float = 1.0, balance = None):\n",
    "    return svm.SVC(kernel = k, C = c,  class_weight= balance).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n",
      "<ipython-input-20-18a030370a7f>:5: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return TP / (TP + FP)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-30ca69b9e474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSVM_M_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     model = [create_SVM(X_train_scaled, y_train, 'linear', c, None), \n\u001b[0m\u001b[1;32m      6\u001b[0m              create_SVM(X_train_scaled, y_train, 'linear', c, 'balanced')]\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-36e08a361f58>\u001b[0m in \u001b[0;36mcreate_SVM\u001b[0;34m(X, y, k, c, balance)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_SVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbalance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_probB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c_list = np.arange(0.1, 5.0, 0.1)\n",
    "SVM_M = []\n",
    "SVM_M_balanced = []\n",
    "for c in c_list:\n",
    "    model = [create_SVM(X_train_scaled, y_train, 'linear', c, None), \n",
    "             create_SVM(X_train_scaled, y_train, 'linear', c, 'balanced')]\n",
    "    \n",
    "    y_pred = [model[0].predict(X_test_scaled),model[1].predict(X_test_scaled)] \n",
    "    \n",
    "    acc = [accuracy(y_test, y_pred[0]), accuracy(y_test, y_pred[1])]\n",
    "    pre = [precision(y_test, y_pred[0]), precision(y_test, y_pred[1])]\n",
    "    rec = [recall(y_test, y_pred[0]), recall(y_test, y_pred[1])]\n",
    "    f1s = [f1(y_test, y_pred[0]), f1(y_test, y_pred[1])]\n",
    "    \n",
    "    SVM_M.append([c, acc[0], pre[0], rec[0] ,f1s[0]])\n",
    "    SVM_M_balanced.append([c, acc[1], pre[1], rec[1] ,f1s[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalanced data: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reg param</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.7</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.8</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.9</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.1</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.2</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.3</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.4</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.6</td>\n",
       "      <td>0.893593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.7</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.8</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.9</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.1</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.2</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.3</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.4</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.6</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3.7</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.8</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3.9</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.1</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.2</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.3</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.6</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.7</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.8</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.892449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reg param  Accuracy  Precision  Recall  f1\n",
       "0         0.1  0.893593        NaN     0.0 NaN\n",
       "1         0.2  0.893593        NaN     0.0 NaN\n",
       "2         0.3  0.893593        NaN     0.0 NaN\n",
       "3         0.4  0.893593        NaN     0.0 NaN\n",
       "4         0.5  0.893593        NaN     0.0 NaN\n",
       "5         0.6  0.893593        NaN     0.0 NaN\n",
       "6         0.7  0.893593        NaN     0.0 NaN\n",
       "7         0.8  0.893593        NaN     0.0 NaN\n",
       "8         0.9  0.893593        NaN     0.0 NaN\n",
       "9         1.0  0.893593        NaN     0.0 NaN\n",
       "10        1.1  0.893593        NaN     0.0 NaN\n",
       "11        1.2  0.893593        NaN     0.0 NaN\n",
       "12        1.3  0.893593        NaN     0.0 NaN\n",
       "13        1.4  0.893593        NaN     0.0 NaN\n",
       "14        1.5  0.893593        NaN     0.0 NaN\n",
       "15        1.6  0.893593        NaN     0.0 NaN\n",
       "16        1.7  0.893593        NaN     0.0 NaN\n",
       "17        1.8  0.893593        NaN     0.0 NaN\n",
       "18        1.9  0.893593        NaN     0.0 NaN\n",
       "19        2.0  0.893593        NaN     0.0 NaN\n",
       "20        2.1  0.893593        NaN     0.0 NaN\n",
       "21        2.2  0.893593        NaN     0.0 NaN\n",
       "22        2.3  0.893593        NaN     0.0 NaN\n",
       "23        2.4  0.893593        NaN     0.0 NaN\n",
       "24        2.5  0.893593        NaN     0.0 NaN\n",
       "25        2.6  0.893593        NaN     0.0 NaN\n",
       "26        2.7  0.892449        0.0     0.0 NaN\n",
       "27        2.8  0.892449        0.0     0.0 NaN\n",
       "28        2.9  0.892449        0.0     0.0 NaN\n",
       "29        3.0  0.892449        0.0     0.0 NaN\n",
       "30        3.1  0.892449        0.0     0.0 NaN\n",
       "31        3.2  0.892449        0.0     0.0 NaN\n",
       "32        3.3  0.892449        0.0     0.0 NaN\n",
       "33        3.4  0.892449        0.0     0.0 NaN\n",
       "34        3.5  0.892449        0.0     0.0 NaN\n",
       "35        3.6  0.892449        0.0     0.0 NaN\n",
       "36        3.7  0.892449        0.0     0.0 NaN\n",
       "37        3.8  0.892449        0.0     0.0 NaN\n",
       "38        3.9  0.892449        0.0     0.0 NaN\n",
       "39        4.0  0.892449        0.0     0.0 NaN\n",
       "40        4.1  0.892449        0.0     0.0 NaN\n",
       "41        4.2  0.892449        0.0     0.0 NaN\n",
       "42        4.3  0.892449        0.0     0.0 NaN\n",
       "43        4.4  0.892449        0.0     0.0 NaN\n",
       "44        4.5  0.892449        0.0     0.0 NaN\n",
       "45        4.6  0.892449        0.0     0.0 NaN\n",
       "46        4.7  0.892449        0.0     0.0 NaN\n",
       "47        4.8  0.892449        0.0     0.0 NaN\n",
       "48        4.9  0.892449        0.0     0.0 NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Balanced data: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reg param</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.582380</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.234801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.581236</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.234310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.576659</td>\n",
       "      <td>0.143959</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.232365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.572082</td>\n",
       "      <td>0.142494</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.230453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.567506</td>\n",
       "      <td>0.141058</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.228571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.561785</td>\n",
       "      <td>0.141089</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.229376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.557208</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.224449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.551487</td>\n",
       "      <td>0.136253</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.546911</td>\n",
       "      <td>0.131387</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.544622</td>\n",
       "      <td>0.130751</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.213439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.545767</td>\n",
       "      <td>0.131068</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.213861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.544622</td>\n",
       "      <td>0.130751</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.213439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.544622</td>\n",
       "      <td>0.130751</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.213439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.544622</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.216535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.132212</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.216110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.545767</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.220039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.7</td>\n",
       "      <td>0.542334</td>\n",
       "      <td>0.133652</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.8</td>\n",
       "      <td>0.541190</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.218324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.9</td>\n",
       "      <td>0.540046</td>\n",
       "      <td>0.133017</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.541190</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.218324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.1</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.2</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.3</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.4</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.540046</td>\n",
       "      <td>0.133017</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.6</td>\n",
       "      <td>0.540046</td>\n",
       "      <td>0.133017</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.7</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.8</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.9</td>\n",
       "      <td>0.540046</td>\n",
       "      <td>0.133017</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.538902</td>\n",
       "      <td>0.132701</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.1</td>\n",
       "      <td>0.537757</td>\n",
       "      <td>0.132388</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.217054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.2</td>\n",
       "      <td>0.536613</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.3</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.4</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.6</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3.7</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.215385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.8</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3.9</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.216216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.212355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.1</td>\n",
       "      <td>0.530892</td>\n",
       "      <td>0.128806</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.2</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.3</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.6</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.212355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.7</td>\n",
       "      <td>0.532037</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.211946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.8</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.215385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.215385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Reg param  Accuracy  Precision    Recall        f1\n",
       "0         0.1  0.582380   0.145833  0.602151  0.234801\n",
       "1         0.2  0.581236   0.145455  0.602151  0.234310\n",
       "2         0.3  0.576659   0.143959  0.602151  0.232365\n",
       "3         0.4  0.572082   0.142494  0.602151  0.230453\n",
       "4         0.5  0.567506   0.141058  0.602151  0.228571\n",
       "5         0.6  0.561785   0.141089  0.612903  0.229376\n",
       "6         0.7  0.557208   0.137931  0.602151  0.224449\n",
       "7         0.8  0.551487   0.136253  0.602151  0.222222\n",
       "8         0.9  0.546911   0.131387  0.580645  0.214286\n",
       "9         1.0  0.544622   0.130751  0.580645  0.213439\n",
       "10        1.1  0.545767   0.131068  0.580645  0.213861\n",
       "11        1.2  0.544622   0.130751  0.580645  0.213439\n",
       "12        1.3  0.544622   0.130751  0.580645  0.213439\n",
       "13        1.4  0.544622   0.132530  0.591398  0.216535\n",
       "14        1.5  0.543478   0.132212  0.591398  0.216110\n",
       "15        1.6  0.545767   0.134615  0.602151  0.220039\n",
       "16        1.7  0.542334   0.133652  0.602151  0.218750\n",
       "17        1.8  0.541190   0.133333  0.602151  0.218324\n",
       "18        1.9  0.540046   0.133017  0.602151  0.217899\n",
       "19        2.0  0.541190   0.133333  0.602151  0.218324\n",
       "20        2.1  0.538902   0.132701  0.602151  0.217476\n",
       "21        2.2  0.538902   0.132701  0.602151  0.217476\n",
       "22        2.3  0.538902   0.132701  0.602151  0.217476\n",
       "23        2.4  0.538902   0.132701  0.602151  0.217476\n",
       "24        2.5  0.540046   0.133017  0.602151  0.217899\n",
       "25        2.6  0.540046   0.133017  0.602151  0.217899\n",
       "26        2.7  0.538902   0.132701  0.602151  0.217476\n",
       "27        2.8  0.538902   0.132701  0.602151  0.217476\n",
       "28        2.9  0.540046   0.133017  0.602151  0.217899\n",
       "29        3.0  0.538902   0.132701  0.602151  0.217476\n",
       "30        3.1  0.537757   0.132388  0.602151  0.217054\n",
       "31        3.2  0.536613   0.132075  0.602151  0.216634\n",
       "32        3.3  0.535469   0.131765  0.602151  0.216216\n",
       "33        3.4  0.535469   0.131765  0.602151  0.216216\n",
       "34        3.5  0.535469   0.131765  0.602151  0.216216\n",
       "35        3.6  0.535469   0.131765  0.602151  0.216216\n",
       "36        3.7  0.533181   0.131148  0.602151  0.215385\n",
       "37        3.8  0.535469   0.131765  0.602151  0.216216\n",
       "38        3.9  0.535469   0.131765  0.602151  0.216216\n",
       "39        4.0  0.533181   0.129412  0.591398  0.212355\n",
       "40        4.1  0.530892   0.128806  0.591398  0.211538\n",
       "41        4.2  0.532037   0.129108  0.591398  0.211946\n",
       "42        4.3  0.532037   0.129108  0.591398  0.211946\n",
       "43        4.4  0.532037   0.129108  0.591398  0.211946\n",
       "44        4.5  0.532037   0.129108  0.591398  0.211946\n",
       "45        4.6  0.533181   0.129412  0.591398  0.212355\n",
       "46        4.7  0.532037   0.129108  0.591398  0.211946\n",
       "47        4.8  0.533181   0.131148  0.602151  0.215385\n",
       "48        4.9  0.533181   0.131148  0.602151  0.215385"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "SVM_table = pd.DataFrame(M, columns = ['Reg param','Accuracy','Precision', 'Recall', 'f1'])\n",
    "SVM_table_balanced = pd.DataFrame(M_balanced, columns = ['Reg param','Accuracy','Precision', 'Recall', 'f1'])\n",
    "\n",
    "print(\"Imbalanced data: \")\n",
    "display(SVM_table)\n",
    "print(\"\\n\")\n",
    "print(\"Balanced data: \")\n",
    "display(SVM_table_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "hte-UpsCJF0Q",
    "nbgrader": {
     "checksum": "2412fc015a280623635f3bf03e3fde9e",
     "grade": true,
     "grade_id": "cell-b4ae750d1154f837",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**Replace the text in this cell with your report**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YI7doqzLJF0P",
    "nbgrader": {
     "checksum": "fdac93da6191896d74c7346d2e875a84",
     "grade": false,
     "grade_id": "cell-4dc578274728380b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Neural Network (Lecture 5) (2pts)\n",
    "\n",
    "+ Train and test a Neural Network model\n",
    "    + Construct a table with each row being a different configuration of the network (play with the number of hidden layers, the number of neurons in each layer, and the activation function) and each column the evaluation measures\n",
    "    + Report the performance of at least three different configurations\n",
    "    + Explain your findings and select the optimal model\n",
    "    + Justify your choice of different paramteres and architectures\n",
    "    + Report the performance of the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "YWgD6iFmJF0Q",
    "nbgrader": {
     "checksum": "e4d7a215763017f7111a6162d214ed5e",
     "grade": true,
     "grade_id": "cell-7009d775455986ad",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "Pw5mJ1z1JF0Q",
    "nbgrader": {
     "checksum": "36cda951bae6da0700d3f6202d0f1a89",
     "grade": true,
     "grade_id": "cell-42c8bf3a6a671fef",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**Replace the text in this cell with your report**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OIgJSXMTJF0Q",
    "nbgrader": {
     "checksum": "baabfebe46559e2d055cef7ceae38b0e",
     "grade": false,
     "grade_id": "cell-23280b034d299667",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Compare Algorithms (2pts)\n",
    "* Plot the Precision-Recall curves for the best model for each one of the above algorithms, Logistic Regression, Neural Nets, and SVM.\n",
    "    * Use the precision_recall_curve from scikit-learn\n",
    "* Explain your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "8kRDcyLuJF0Q",
    "nbgrader": {
     "checksum": "b17597ab95452fa3d4259cb4d2bee6c2",
     "grade": true,
     "grade_id": "cell-d6967e3b3e3dbe7a",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "_vShAj6OJF0Q",
    "nbgrader": {
     "checksum": "58ca34644cf139f5987905e2058ea0b3",
     "grade": true,
     "grade_id": "cell-b91319845bc74219",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**Replace the text in this cell with your report**</span>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "Practical Assignment 2 - Student.ipynb ",
   "provenance": [
    {
     "file_id": "17ja4GM2Ci0MjQjO3RjTNBS0wcnLBpBIr",
     "timestamp": 1618862623296
    },
    {
     "file_id": "1tO5F-kPHZPpMGLNuPjfNqd3rXU1Lkmn1",
     "timestamp": 1618842712618
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
